{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'NVIDIA GeForce RTX 2080 SUPER'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll learn how to manipulate tensors using Pytorch tensor library.\n",
    "- how the data is stored in memory\n",
    "- how certain operations can be performed on large tensors \n",
    "- numpy interoperability\n",
    "- gpu acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1.0, 2.3, 4.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 1., 1.])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3) # one-dim tensor size 3 filled with 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python objects are stored in memory, tensors in pytorch are stored in unboxed C numeric types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([4., 1., 5., 3., 2., 1.])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create coordinates of a triangle 2D\n",
    "points = torch.zeros(6)\n",
    "points[0] = 4.0\n",
    "points[1] = 1.0\n",
    "points[2] = 5.0\n",
    "points[3] = 3.0\n",
    "points[4] = 2.0\n",
    "points[5] = 1.0\n",
    "\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 1.],\n        [5., 3.],\n        [2., 1.]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or passing as coordinates\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 2])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the tensor\n",
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0.],\n        [0., 0.],\n        [0., 0.]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a tensor with specific dimensions\n",
    "points = torch.zeros(3, 2)\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 3]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list = list(range(10))\n",
    "some_list[1:4:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0:3:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch also have an advanced indexing, used as a powerful feature between others developers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named tensors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions (or axes) of our tensor usually index something like pixel locations or color channels. This means when we want to index into a tensor, we need to remember the ordering of the dimensions and write our indexing accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]\n",
    "weights = torch.tensor([0.2126, 0.78, 0.0722])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[-7.0865e-01,  9.7672e-01, -6.3954e-01,  1.5766e-01,  1.1037e+00],\n          [ 1.5463e+00,  1.2739e-01, -7.4799e-01, -2.6169e-01, -1.0695e+00],\n          [ 4.0478e-01, -1.1229e+00,  1.1241e+00, -5.7383e-01, -7.2641e-01],\n          [-7.1018e-02, -2.8328e-01, -9.0021e-01,  9.9194e-01, -1.3993e+00],\n          [ 2.0287e+00, -7.1575e-01,  4.9782e-01,  1.5286e+00, -1.7222e+00]],\n\n         [[ 1.3631e+00, -1.0093e+00,  5.0792e-01, -1.3808e+00, -8.2902e-01],\n          [ 2.2875e-01, -7.7122e-01,  1.2225e+00,  1.7267e+00,  1.4250e+00],\n          [ 1.7175e+00, -9.4228e-01,  4.8240e-01, -1.9607e+00,  2.3667e-01],\n          [ 1.5387e-01, -6.2021e-01, -4.6766e-01, -2.6442e-01,  4.2527e-01],\n          [ 1.3664e-01, -1.9370e+00,  2.8235e-01, -4.5833e-02,  4.6699e-01]],\n\n         [[ 6.7786e-01, -9.8803e-03,  4.3896e-02, -1.4911e+00, -1.2718e+00],\n          [-1.7003e+00,  9.7886e-01, -7.2707e-01,  3.1534e-01, -7.2268e-01],\n          [-6.8551e-01,  1.5914e-01, -3.2088e-01,  6.1872e-01, -2.9760e-01],\n          [-1.4667e+00,  7.7987e-01, -1.4215e+00,  1.7281e-01,  3.3415e-01],\n          [-1.0980e-01, -2.0864e+00, -1.4995e+00,  9.3090e-01, -9.8423e-01]]],\n\n\n        [[[-7.7818e-01,  2.1240e-01, -2.5575e-01, -1.1207e-01,  3.0297e-01],\n          [ 1.2307e-01,  2.5682e-01,  1.3860e+00, -6.9944e-01,  3.2660e-01],\n          [ 9.3778e-01, -7.7880e-01, -4.3544e-01,  8.7340e-01,  1.8007e+00],\n          [ 5.9516e-01,  6.4743e-01,  7.0156e-01,  5.8675e-01,  4.3594e-01],\n          [ 7.8211e-01, -7.9054e-01, -6.2969e-01,  1.5007e+00, -2.1642e+00]],\n\n         [[-5.8203e-01,  7.7279e-02,  1.0925e+00, -7.7620e-01,  3.2062e-01],\n          [ 1.1308e+00, -1.6772e-01, -1.2882e-01, -8.5065e-01, -1.5459e+00],\n          [-1.2291e+00, -1.5875e-01,  7.5338e-01, -1.1904e+00, -5.5750e-01],\n          [-1.2725e+00, -6.4576e-01, -1.3828e+00, -7.7160e-02, -1.1448e+00],\n          [ 5.3516e-01, -4.3604e-04, -1.1714e+00,  2.6314e-01,  1.6712e+00]],\n\n         [[ 1.1959e-01, -1.2729e-01,  2.1645e-01,  5.0785e-01, -1.0622e+00],\n          [ 6.4918e-01,  1.2442e+00, -1.3991e+00,  2.8417e-01,  2.6429e-01],\n          [-2.0252e-01,  2.3267e-01,  2.3116e-01,  4.0131e-01, -3.0917e-01],\n          [-3.0868e+00, -1.9265e+00, -4.9271e-01, -1.1574e+00, -7.1646e-01],\n          [ 4.8437e-01,  5.2248e-01,  4.2735e-01,  5.1595e-01, -1.2420e-01]]]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch 1.3 introduced a named tensors. **Tensor factory** functions such as **tensors** and **rand** take a names argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0857, -0.3917,  0.3448,  0.4952,  0.3418],\n        [ 0.4057,  1.0863,  0.9266, -0.0197,  0.2515],\n        [-0.4586,  0.5306, -0.0468,  0.5962,  1.0785],\n        [ 0.0461,  0.9818,  0.7190, -1.1122, -0.5134],\n        [-0.3320, -0.4755,  0.3437, -0.4042,  0.1216]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.mean(-3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 5, 5])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Named tensors\n",
    "\n",
    "As data is transformed through multiple tensors, keeping track of which dimension contains what data can be error-prone."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# imagine a 3D image, 3D tensor, and we want to convert it to grayscale.\n",
    "img_t = torch.randn(3, 5, 5)\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# we also want to generalize, from a 2D grayscale, add a third dim to add color.\n",
    "# adding a batch_t.\n",
    "batch_t = torch.randn(2, 3, 5, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so RGB channels are always on dimension -3.\n",
    "img_gray_naive = img_t.mean(-3)\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another step of the process\n",
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\n",
    "img_weights = (img_t * unsqueezed_weights)\n",
    "batch_weights = (batch_t * unsqueezed_weights)\n",
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_weights.shape, batch_t.shape, unsqueezed_weights.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "...This is a too complicated and error prune process. For that we use named tensors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=[\"channels\"])\n",
    "weights_named"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.0947, -0.7375, -0.3886,  0.2595, -1.6724],\n         [-1.2216, -0.1523, -1.3980, -0.2321,  1.0383],\n         [-0.8778, -0.1185, -1.0319,  0.3926,  0.2442],\n         [-1.3668, -1.7488,  0.6225,  0.4324, -0.0553],\n         [-0.5542, -2.6776,  0.6894,  1.6461, -0.7349]],\n\n        [[-0.5235, -2.0107, -0.2135, -3.2689, -2.4123],\n         [-1.9856, -1.1923, -0.4498,  0.5827, -0.1437],\n         [ 0.1517, -0.1497, -0.2720, -2.4248,  0.4925],\n         [-1.2434,  2.8643,  1.6844,  0.1003, -0.3706],\n         [-0.5419, -1.7610, -2.4077,  0.0674,  0.0040]],\n\n        [[ 0.0806, -0.1083, -1.0673, -1.0480, -0.1287],\n         [ 0.1852, -2.0077,  0.9057, -1.4238, -1.2967],\n         [-1.3148, -0.9351, -0.6671,  0.1097,  1.7359],\n         [-1.6605,  1.8171,  0.9547,  0.3827,  0.1359],\n         [ 0.6059, -0.6129, -0.6666, -0.5589, -1.5323]]],\n       names=('channels', 'rows', 'columns'))"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to add names to a existing tensor, use .refine_names()\n",
    "img_named = img_t.refine_names(..., \"channels\", \"rows\", \"columns\")\n",
    "batch_named = batch_t.refine_names(..., \"channels\", \"rows\", \"columns\")\n",
    "img_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[-0.1649, -1.2870, -1.1137,  0.3297,  0.7800],\n          [-1.5436, -0.7299,  2.3760, -0.8542,  1.2475],\n          [ 1.0200, -2.3685, -2.0618, -0.5795,  1.3302],\n          [ 0.1735, -1.4492, -1.1893,  1.3111,  1.7122],\n          [ 0.7962,  0.7818,  0.7990,  0.7661,  0.8400]],\n\n         [[ 0.5923,  0.5611, -0.6370, -0.6348,  0.6865],\n          [-1.8531,  1.5370, -0.8730, -0.3203, -0.9351],\n          [ 1.3536, -1.1781, -0.5665, -0.2706, -0.1327],\n          [-0.3308,  1.4281, -0.8178, -0.0404,  1.7440],\n          [ 2.1795,  0.1132, -1.3456, -0.4383, -0.0717]],\n\n         [[ 0.3562,  0.4753, -1.4866,  0.3955,  1.7102],\n          [ 0.4515,  1.6538,  1.2416, -0.4988, -0.0052],\n          [-0.6367, -1.6565, -1.3755, -0.7261,  1.5513],\n          [ 0.2134,  0.2365,  0.7124, -1.8809,  0.3725],\n          [-0.1931, -1.6686,  0.1771, -0.2743, -0.2706]]],\n\n\n        [[[-0.0385, -0.1041, -0.1724,  1.4859, -1.5508],\n          [-0.2985, -0.1178, -0.8857, -0.6669, -0.3411],\n          [-0.2994,  0.5108,  0.2367,  0.7637,  0.3131],\n          [ 0.3932, -0.3242, -0.9439,  0.6667,  0.1114],\n          [ 0.8993, -2.6887, -1.4464,  0.3440, -0.0181]],\n\n         [[ 0.2871,  0.6931,  0.7047, -1.1589, -0.6788],\n          [-0.7754,  1.1529, -1.6626, -0.8816,  1.3092],\n          [-0.6890, -1.2390, -0.6609, -0.3117, -0.9301],\n          [ 1.4776,  0.1639, -0.8701,  2.2994,  0.6416],\n          [ 0.5597,  1.5262,  1.0610, -0.9143,  0.6881]],\n\n         [[ 0.7402,  0.3456, -1.8915,  0.3854, -0.5808],\n          [ 0.1058, -0.4544,  0.2170,  0.0717,  0.4468],\n          [-0.6302, -0.0470, -0.1005,  1.9777, -0.4014],\n          [ 1.2031,  0.6534,  0.4375,  1.1809,  0.6177],\n          [ 1.4074, -0.7590, -0.0544,  0.1469, -0.1724]]]],\n       names=(None, 'channels', 'rows', 'columns'))"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_named"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pytorch will now check the names for us on two inputs operations. So far, it does not automatically align dimensions, so we need to do this explicitly. The method align_as returns a tensor with missing dimensions added and existing ones permuted to the right order"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.2126]],\n\n        [[0.7152]],\n\n        [[0.0722]]], names=('channels', 'rows', 'columns'))"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named)\n",
    "weights_aligned"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.3485, -1.6026, -0.3124, -2.3584, -2.0901],\n        [-1.6665, -1.0300, -0.5535,  0.2646,  0.0243],\n        [-0.1730, -0.1998, -0.4621, -1.6429,  0.5295],\n        [-1.2997,  1.8080,  1.4059,  0.1913, -0.2670],\n        [-0.4617, -1.8730, -1.6235,  0.3578, -0.2640]],\n       names=('rows', 'columns'))"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some operations also take named dimensions as input\n",
    "(img_named * weights_aligned).sum('channels')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensor Element types\n",
    "\n",
    "Python numerical types are suboptimal, and for that data science libraries rely on numpy or others dedicated data structures to improve efficience.\n",
    "\n",
    "## Numeric types\n",
    "\n",
    "- torch.float32\n",
    "- torch.float64 or touch.double\n",
    "- torch.int8\n",
    "- torch.bool\n",
    "- ...\n",
    "\n",
    "As we'll see, computations happening in nns are typically executed with 32-bit floating-point precision. 64-bits will not improve performance.\n",
    "\n",
    "16-bit precision can be used on some cpus and gpus."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Managing a tensor's dtype attribute"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2],\n        [3, 4]], dtype=torch.int16)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_points = torch.ones(10, 2, dtype=torch.double)\n",
    "short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)\n",
    "short_points"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.]], dtype=torch.float64)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_points"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.]], dtype=torch.float64)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also casting the result of some operations on tensors\n",
    "torch.zeros(10, 2).double()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0]], dtype=torch.int16)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or using to method\n",
    "torch.zeros(19, 2).to(torch.short)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When mixing input types in operations, the inputs are converted to the larger type automatically."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The tensor API\n",
    "\n",
    "Some basic examples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1.],\n        [1., 1., 1.]])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a_t = torch.transpose(a, 0, 1)\n",
    "a_t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensors: Scenic views of storage\n",
    "\n",
    "- Low level understanding of how the tensors works\n",
    "\n",
    "Values in tensors are allocated in contiguous chunks of memory managed by `torch.Storage` instances.\n",
    "\n",
    "A storage is a one-dimensional array of numerical data: that is, a contiguous block of memory containing numbers of a given type\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "082e8113877ca904c9cb73c189faa563f4d929c3b57e0348f57b959951475961"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('3.9.0': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}