{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2080 SUPER'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll learn how to manipulate tensors using Pytorch tensor library.\n",
    "- how the data is stored in memory\n",
    "- how certain operations can be performed on large tensors \n",
    "- numpy interoperability\n",
    "- gpu acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1.0, 2.3, 4.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3) # one-dim tensor size 3 filled with 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python objects are stored in memory, tensors in pytorch are stored in unboxed C numeric types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1., 5., 3., 2., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create coordinates of a triangle 2D\n",
    "points = torch.zeros(6)\n",
    "points[0] = 4.0\n",
    "points[1] = 1.0\n",
    "points[2] = 5.0\n",
    "points[3] = 3.0\n",
    "points[4] = 2.0\n",
    "points[5] = 1.0\n",
    "\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or passing as coordinates\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the tensor\n",
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a tensor with specific dimensions\n",
    "points = torch.zeros(3, 2)\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list = list(range(10))\n",
    "some_list[1:4:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0:3:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch also have an advanced indexing, used as a powerful feature between others developers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named tensors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions (or axes) of our tensor usually index something like pixel locations or color channels. This means when we want to index into a tensor, we need to remember the ordering of the dimensions and write our indexing accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]\n",
    "weights = torch.tensor([0.2126, 0.78, 0.0722])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.7819,  0.5808, -1.0045,  0.9410,  1.9312],\n",
       "          [-0.8312, -1.2887,  0.1066,  0.5999,  0.1806],\n",
       "          [-1.6966, -0.6013,  0.9279, -1.2569,  0.9823],\n",
       "          [ 1.6346,  0.0363,  2.1404,  0.8002, -0.1722],\n",
       "          [-1.2858,  0.2372,  1.1009, -0.4881,  0.4264]],\n",
       "\n",
       "         [[-1.6378, -0.8108,  1.0617, -1.4675, -0.7600],\n",
       "          [-1.6526, -1.0222, -1.3802,  0.4912,  1.3696],\n",
       "          [ 1.0801,  0.7982,  0.2925, -0.0907,  0.1693],\n",
       "          [ 0.5236, -0.2257, -0.8769, -1.3331,  0.3376],\n",
       "          [-0.0588,  0.8637, -0.4297, -0.9982,  0.5919]],\n",
       "\n",
       "         [[-0.4046, -0.4428, -0.2595,  0.0892,  0.2636],\n",
       "          [-0.7281,  0.4554,  0.6661, -1.5187, -0.7507],\n",
       "          [ 1.2599, -2.6768, -0.3505, -0.7093,  2.1740],\n",
       "          [-1.4718, -1.3350, -0.4883,  2.2749,  0.8795],\n",
       "          [ 0.4287, -0.4178,  0.5689,  0.6871, -0.0300]]],\n",
       "\n",
       "\n",
       "        [[[-0.2983,  2.7361,  1.7560, -0.8485, -0.2876],\n",
       "          [ 0.6648,  0.2645, -0.0536,  0.1974, -0.2033],\n",
       "          [-1.6270, -0.3321, -0.2514,  1.1932, -0.6235],\n",
       "          [-1.8798, -2.0279, -0.4775,  0.7315,  1.2552],\n",
       "          [ 0.3005,  0.1465, -0.5850, -0.0821,  0.3123]],\n",
       "\n",
       "         [[-0.1617,  0.5844,  0.3491, -0.9916,  0.3812],\n",
       "          [ 2.3086, -0.6441,  0.8207,  1.4439, -0.2707],\n",
       "          [-0.0119,  0.7394, -0.2021, -1.0651, -0.6820],\n",
       "          [-0.6458,  0.0766, -0.1228, -0.7616, -0.0136],\n",
       "          [ 1.8258,  0.7675,  0.1337, -0.5329,  0.5954]],\n",
       "\n",
       "         [[-1.2618, -1.3880, -1.0931, -0.1619,  2.2715],\n",
       "          [-0.4351,  1.0491, -1.0543,  1.1345, -0.5781],\n",
       "          [-1.1427,  1.2443,  0.3528,  0.0634,  0.1406],\n",
       "          [-0.1862, -1.1760,  0.9577,  0.6648, -0.7675],\n",
       "          [-0.3785, -1.6463,  0.3957,  0.1339,  1.7132]]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch 1.3 introduced a named tensors. **Tensor factory** functions such as **tensors** and **rand** take a names argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8638/1321878458.py:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1408.)\n",
      "  Weights_named = torch.tensor([0.2125, 0.755, 0.0345], names=[\"channels\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2125, 0.7550, 0.0345], names=('channels',))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weights_named = torch.tensor([0.2125, 0.755, 0.0345], names=[\"channels\"])\n",
    "Weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9455,  0.9939,  0.4753, -0.3491,  1.0914],\n",
       "         [-0.2602,  1.3988, -1.7387, -0.0784, -0.2512],\n",
       "         [ 2.4587,  1.1470, -0.2353, -0.3455, -1.0662],\n",
       "         [-0.1570, -1.5262, -0.6244,  0.8368,  0.9062],\n",
       "         [-1.5603,  0.4683,  0.4650,  1.8803,  0.0094]],\n",
       "\n",
       "        [[ 0.3564,  0.4882,  0.5638, -0.8486, -1.0810],\n",
       "         [-0.5640,  0.2530,  0.0352,  0.9452,  0.0673],\n",
       "         [-0.2041,  0.4780, -0.7393,  1.4690, -0.2960],\n",
       "         [ 0.1271, -1.2029, -2.4153, -1.2643,  0.7097],\n",
       "         [ 0.7378,  0.6264,  1.3681, -0.3487, -1.2724]],\n",
       "\n",
       "        [[-0.4709, -0.4903,  2.9442,  0.6665, -0.6970],\n",
       "         [-1.3665,  0.3001, -1.3248,  0.7635,  0.0216],\n",
       "         [ 0.4380,  2.7411,  0.0673, -1.5146,  0.0333],\n",
       "         [-1.6363,  0.3584, -0.8034, -2.2320,  0.2563],\n",
       "         [ 0.1360, -1.2189,  1.0704,  0.6455,  1.5432]]],\n",
       "       names=('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named = img_t.refine_names(..., \"channels\", \"rows\", \"columns\")\n",
    "img_named"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named tensors have the potential to eliminate many sources of alignment errors which can be a source of headaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor element types\n",
    "\n",
    "What kinds of numeric types we can store in a Tensor? Python numeric types can be suboptimal for several reasons:\n",
    "\n",
    "- Numbers in python are objects: stored with a boxing operation\n",
    "- Lists in python are meant for sequential collections of objects\n",
    "- Python interpreter is slow compared to optimized, compiled code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch dtypes\n",
    "torch.float\n",
    "torch.float32\n",
    "torch.float64\n",
    "torch.half\n",
    "torch.int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "082e8113877ca904c9cb73c189faa563f4d929c3b57e0348f57b959951475961"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('3.9.0': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
